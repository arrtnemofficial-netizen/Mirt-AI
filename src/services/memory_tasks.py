"""
Memory Background Tasks - Time Decay & Hygiene.
================================================
–§–æ–Ω–æ–≤—ñ –∑–∞–¥–∞—á—ñ –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∑–¥–æ—Ä–æ–≤–æ—ó –ø–∞–º º—è—Ç—ñ:

1. apply_time_decay - –∑–º–µ–Ω—à—É—î importance —Å—Ç–∞—Ä–∏—Ö —Ñ–∞–∫—Ç—ñ–≤
2. cleanup_expired - –≤–∏–¥–∞–ª—è—î expired —Ñ–∞–∫—Ç–∏
3. generate_summaries - –≥–µ–Ω–µ—Ä—É—î summary –¥–ª—è –∞–∫—Ç–∏–≤–Ω–∏—Ö —é–∑–µ—Ä—ñ–≤
4. memory_maintenance - –ø–æ–≤–Ω–∏–π —Ü–∏–∫–ª –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è

–¶—ñ –∑–∞–¥–∞—á—ñ –º–∞—é—Ç—å –∑–∞–ø—É—Å–∫–∞—Ç–∏—Å—è —á–µ—Ä–µ–∑ cron –∞–±–æ Celery beat.

–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π —Ä–æ–∑–∫–ª–∞–¥:
- apply_time_decay: —â–æ–¥–Ω—è –æ 3:00
- cleanup_expired: —â–æ–¥–Ω—è –æ 4:00
- generate_summaries: —â–æ—Ç–∏–∂–Ω—è (–Ω–µ–¥—ñ–ª—è –æ 5:00)
"""

from __future__ import annotations

import asyncio
import logging
from datetime import UTC, datetime, timedelta

try:
    import psycopg
    from psycopg.rows import dict_row
except ImportError:
    psycopg = None  # type: ignore
    dict_row = None  # type: ignore

from src.services.memory_service import MemoryService
from src.services.postgres_pool import get_postgres_url


logger = logging.getLogger(__name__)


# =============================================================================
# CORE MAINTENANCE TASKS
# =============================================================================


async def apply_time_decay() -> dict[str, int]:
    """
    –ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ time decay –¥–æ —Å—Ç–∞—Ä–∏—Ö —Ñ–∞–∫—Ç—ñ–≤.

    –ó–º–µ–Ω—à—É—î importance –¥–ª—è —Ñ–∞–∫—Ç—ñ–≤, —è–∫—ñ –¥–∞–≤–Ω–æ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏—Å—å.
    –¶–µ —Ä–æ–±–∏—Ç—å –ø–∞–º º—è—Ç—å "–∂–∏–≤–æ—é" ‚Äî –∑–∞—Å—Ç–∞—Ä—ñ–ª–µ –ø–æ—Å—Ç—É–ø–æ–≤–æ –∑–∞–±—É–≤–∞—î—Ç—å—Å—è.

    Returns:
        Stats dict with affected counts
    """
    logger.info("üïê Starting memory time decay...")
    start = datetime.now(UTC)

    memory_service = MemoryService()

    if not memory_service.enabled:
        logger.warning("Memory service disabled, skipping time decay")
        return {"affected": 0, "error": "disabled"}

    try:
        affected = await memory_service.apply_time_decay()

        elapsed = (datetime.now(UTC) - start).total_seconds()
        logger.info("‚úÖ Time decay complete: %d facts affected in %.2fs", affected, elapsed)

        return {
            "affected": affected,
            "elapsed_seconds": elapsed,
            "timestamp": start.isoformat(),
        }

    except Exception as e:
        logger.error("‚ùå Time decay failed: %s", e)
        return {"affected": 0, "error": str(e)}


async def cleanup_expired() -> dict[str, int]:
    """
    –í–∏–¥–∞–ª–∏—Ç–∏ (–¥–µ–∞–∫—Ç–∏–≤—É–≤–∞—Ç–∏) expired —Ñ–∞–∫—Ç–∏.

    –§–∞–∫—Ç–∏ –∑ expires_at < now() –±—É–¥—É—Ç—å –ø–æ–º—ñ—á–µ–Ω—ñ —è–∫ is_active=False.

    Returns:
        Stats dict with cleaned counts
    """
    logger.info("üßπ Starting expired memories cleanup...")
    start = datetime.now(UTC)

    memory_service = MemoryService()

    if not memory_service.enabled:
        logger.warning("Memory service disabled, skipping cleanup")
        return {"cleaned": 0, "error": "disabled"}

    try:
        cleaned = await memory_service.cleanup_expired()

        elapsed = (datetime.now(UTC) - start).total_seconds()
        logger.info("‚úÖ Cleanup complete: %d expired facts deactivated in %.2fs", cleaned, elapsed)

        return {
            "cleaned": cleaned,
            "elapsed_seconds": elapsed,
            "timestamp": start.isoformat(),
        }

    except Exception as e:
        logger.error("‚ùå Cleanup failed: %s", e)
        return {"cleaned": 0, "error": str(e)}


async def generate_user_summary(user_id: str) -> dict[str, any]:
    """
    –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ summary –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

    –ó–±–∏—Ä–∞—î –≤—Å—ñ –∞–∫—Ç–∏–≤–Ω—ñ —Ñ–∞–∫—Ç–∏ —ñ —Å—Ç–≤–æ—Ä—é—î —Å—Ç–∏—Å–ª–∏–π summary.

    Args:
        user_id: User to generate summary for

    Returns:
        Summary stats
    """
    memory_service = MemoryService()

    if not memory_service.enabled:
        return {"error": "disabled"}

    try:
        # Get all facts for user
        facts = await memory_service.get_facts(user_id, limit=50, min_importance=0.3)

        if not facts:
            return {"user_id": user_id, "facts_count": 0, "summary": None}

        # Group facts by category
        categories = {}
        for fact in facts:
            cat = fact.category
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(fact.content)

        # Build summary text
        summary_parts = []
        for cat, contents in categories.items():
            summary_parts.append(f"{cat}: {'; '.join(contents[:3])}")

        summary_text = " | ".join(summary_parts)

        # Extract key facts (top 5 by importance)
        key_facts = [f.content for f in sorted(facts, key=lambda x: x.importance, reverse=True)[:5]]

        # Save summary
        result = await memory_service.save_summary(
            user_id=user_id,
            summary_text=summary_text[:500],  # Limit length
            key_facts=key_facts,
            facts_count=len(facts),
        )

        return {
            "user_id": user_id,
            "facts_count": len(facts),
            "summary_saved": result is not None,
        }

    except Exception as e:
        logger.error("Failed to generate summary for user %s: %s", user_id, e)
        return {"user_id": user_id, "error": str(e)}


async def generate_summaries_for_active_users(days: int = 7) -> dict[str, any]:
    """
    –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ summaries –¥–ª—è –∞–∫—Ç–∏–≤–Ω–∏—Ö —é–∑–µ—Ä—ñ–≤.

    Args:
        days: Consider users active if seen in last N days

    Returns:
        Stats dict
    """
    logger.info("üìù Starting summary generation for active users (last %d days)...", days)
    start = datetime.now(UTC)

    if psycopg is None:
        logger.warning("psycopg not installed")
        return {"processed": 0, "error": "no_client"}

    try:
        # Get active users
        cutoff = datetime.now(UTC) - timedelta(days=days)

        url = get_postgres_url()
        with psycopg.connect(url) as conn:
            with conn.cursor(row_factory=dict_row) as cur:
                cur.execute(
                    """
                    SELECT user_id FROM mirt_profiles
                    WHERE last_seen_at >= %s
                    """,
                    (cutoff,),
                )
                rows = cur.fetchall()

        if not rows:
            logger.info("No active users found")
            return {"processed": 0}

        user_ids = [row["user_id"] for row in rows]
        logger.info("Found %d active users", len(user_ids))

        # Generate summaries
        results = []
        for user_id in user_ids:
            result = await generate_user_summary(user_id)
            results.append(result)

        successful = sum(1 for r in results if r.get("summary_saved"))
        elapsed = (datetime.now(UTC) - start).total_seconds()

        logger.info(
            "‚úÖ Summary generation complete: %d/%d users in %.2fs",
            successful,
            len(user_ids),
            elapsed,
        )

        return {
            "processed": len(user_ids),
            "successful": successful,
            "elapsed_seconds": elapsed,
            "timestamp": start.isoformat(),
        }

    except Exception as e:
        logger.error("‚ùå Summary generation failed: %s", e)
        return {"processed": 0, "error": str(e)}


async def memory_maintenance() -> dict[str, any]:
    """
    –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è –ø–∞–º º—è—Ç—ñ.

    –ó–∞–ø—É—Å–∫–∞—î:
    1. Time decay
    2. Cleanup expired
    3. Generate summaries

    Returns:
        Combined stats from all tasks
    """
    logger.info("üîß Starting full memory maintenance cycle...")
    start = datetime.now(UTC)

    results = {}

    # Step 1: Time decay
    results["time_decay"] = await apply_time_decay()

    # Step 2: Cleanup
    results["cleanup"] = await cleanup_expired()

    # Step 3: Summaries (only for users active in last 3 days)
    results["summaries"] = await generate_summaries_for_active_users(days=3)

    elapsed = (datetime.now(UTC) - start).total_seconds()
    results["total_elapsed_seconds"] = elapsed
    results["timestamp"] = start.isoformat()

    logger.info("‚úÖ Full maintenance complete in %.2fs", elapsed)

    return results


# =============================================================================
# NOTE: Celery tasks removed
# =============================================================================
# Celery is now only used for followups and summarization.
# Memory maintenance tasks should be run via CLI or scheduled scripts, not Celery.


# =============================================================================
# CLI ENTRY POINT
# =============================================================================


def run_maintenance_cli():
    """CLI entry point for running maintenance manually."""
    import argparse

    parser = argparse.ArgumentParser(description="MIRT Memory Maintenance")
    parser.add_argument(
        "--task",
        choices=["decay", "cleanup", "summaries", "full"],
        default="full",
        help="Which task to run",
    )
    parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="Days for active user threshold (summaries only)",
    )

    args = parser.parse_args()

    if args.task == "decay":
        result = asyncio.run(apply_time_decay())
    elif args.task == "cleanup":
        result = asyncio.run(cleanup_expired())
    elif args.task == "summaries":
        result = asyncio.run(generate_summaries_for_active_users(args.days))
    else:
        result = asyncio.run(memory_maintenance())

    print(f"Result: {result}")
    return result


if __name__ == "__main__":
    run_maintenance_cli()
